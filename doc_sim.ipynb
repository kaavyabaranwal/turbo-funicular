{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow tensorflow_hub torch transformers nltk gensim scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kaavy\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kaavy\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kaavy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kaavy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import tensorflow_hub as hub\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Download stopwords and tokenizer models if not available\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "articles_df = pd.read_csv('articles_dataset.csv')\n",
    "main_articles_df = pd.read_csv('main_articles.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JACCARD SIMILARITY \n",
    "##### The Jaccard similarity compares the overlap of unique words between two documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(doc1, doc2):\n",
    "    tokens1 = set(word_tokenize(doc1.lower()))\n",
    "    tokens2 = set(word_tokenize(doc2.lower()))\n",
    "    intersection = tokens1.intersection(tokens2)\n",
    "    union = tokens1.union(tokens2)\n",
    "    return len(intersection) / len(union) if union else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Cosine Similarity\n",
    "##### TF-IDF (Term Frequency-Inverse Document Frequency) represents text as weighted terms based on their frequency across documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TF-IDF cosine similarity\n",
    "def tfidf_cosine_similarity(articles, main_articles):\n",
    "    documents = articles + main_articles\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix[len(articles):], tfidf_matrix[:len(articles)])\n",
    "    return similarity_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doc2vec_model(documents):\n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]) for i, doc in enumerate(documents)]\n",
    "    model = Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    return model\n",
    "\n",
    "# Calculate Doc2Vec similarity\n",
    "def doc2vec_similarity(doc2vec_model, main_articles, articles):\n",
    "    similarities = []\n",
    "    \n",
    "    # Get the vectors for the articles in the dataset\n",
    "    article_vectors = [doc2vec_model.infer_vector(word_tokenize(article.lower())) for article in articles]\n",
    "\n",
    "    for main_article in main_articles:\n",
    "        # Infer vector for the main article\n",
    "        main_vector = doc2vec_model.infer_vector(word_tokenize(main_article.lower()))\n",
    "        \n",
    "        # Calculate cosine similarity between the main article vector and each article vector\n",
    "        sims = cosine_similarity([main_vector], article_vectors)[0]\n",
    "        similarities.append(sims)\n",
    "    \n",
    "    return np.array(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kaavy\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kaavy\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kaavy\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kaavy\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load USE model\n",
    "use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# Calculate USE similarity\n",
    "def use_similarity(use_model, main_articles, articles):\n",
    "    embeddings = use_model(articles + main_articles)\n",
    "    article_embeddings = embeddings[:len(articles)]\n",
    "    main_article_embeddings = embeddings[len(articles):]\n",
    "    similarity_matrix = cosine_similarity(main_article_embeddings, article_embeddings)\n",
    "    return similarity_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30297500bd2440ed8eda2dccd9a67a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kaavy\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kaavy\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6037fef7c66c44fd9550fc7a7dd34c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317cafb45ffc47b79f003456169809cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4909574fb845039e90c638fe96632e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load BERT model and tokenizer\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Function to get BERT embeddings\n",
    "def bert_embedding(text):\n",
    "    inputs = bert_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = bert_model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "    return embeddings\n",
    "\n",
    "# Calculate BERT similarity\n",
    "def bert_cosine_similarity(main_articles, articles):\n",
    "    article_embeddings = torch.stack([bert_embedding(doc) for doc in articles])\n",
    "    main_article_embeddings = torch.stack([bert_embedding(doc) for doc in main_articles])\n",
    "    similarity_matrix = cosine_similarity(main_article_embeddings.detach().numpy(), article_embeddings.detach().numpy())\n",
    "    return similarity_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JACCARD\n",
      " [[0.32231404958677684, 0.17647058823529413, 0.234375, 0.24812030075187969, 0.17777777777777778, 0.09859154929577464, 0.06756756756756757, 0.07236842105263158, 0.0896551724137931, 0.08904109589041095, 0.0763888888888889, 0.10714285714285714, 0.10869565217391304, 0.0851063829787234, 0.09859154929577464, 0.1323529411764706, 0.13138686131386862, 0.11267605633802817, 0.07042253521126761, 0.06756756756756757, 0.0763888888888889, 0.09523809523809523, 0.06944444444444445, 0.05517241379310345, 0.08450704225352113, 0.1450381679389313, 0.07801418439716312, 0.06944444444444445, 0.06993006993006994, 0.14492753623188406], [0.08333333333333333, 0.10638297872340426, 0.08450704225352113, 0.08, 0.06896551724137931, 0.35714285714285715, 0.14925373134328357, 0.14388489208633093, 0.16666666666666666, 0.34782608695652173, 0.08633093525179857, 0.10218978102189781, 0.1037037037037037, 0.09558823529411764, 0.10144927536231885, 0.08695652173913043, 0.07857142857142857, 0.06206896551724138, 0.1044776119402985, 0.1079136690647482, 0.14393939393939395, 0.0979020979020979, 0.12781954887218044, 0.09558823529411764, 0.0948905109489051, 0.08148148148148149, 0.1044776119402985, 0.07913669064748201, 0.09558823529411764, 0.09219858156028368], [0.08759124087591241, 0.07194244604316546, 0.05755395683453238, 0.0763888888888889, 0.08823529411764706, 0.09022556390977443, 0.09701492537313433, 0.10144927536231885, 0.10526315789473684, 0.0962962962962963, 0.34579439252336447, 0.18032786885245902, 0.16393442622950818, 0.21367521367521367, 0.16, 0.08333333333333333, 0.08270676691729323, 0.06521739130434782, 0.06818181818181818, 0.08888888888888889, 0.06666666666666667, 0.08695652173913043, 0.07518796992481203, 0.09230769230769231, 0.1349206349206349, 0.07751937984496124, 0.07633587786259542, 0.0916030534351145, 0.07575757575757576, 0.08088235294117647]]\n",
      "Most relevant article to main article 1: Article ID 1, Score: 0.32231404958677684\n",
      "Most relevant article to main article 2: Article ID 6, Score: 0.35714285714285715\n",
      "Most relevant article to main article 3: Article ID 11, Score: 0.34579439252336447\n"
     ]
    }
   ],
   "source": [
    "# Prepare documents\n",
    "article_texts = articles_df['content'].tolist()\n",
    "main_article_texts = main_articles_df['content'].tolist()\n",
    "\n",
    "# Calculate similarities\n",
    "# Jaccard Similarity\n",
    "jaccard_results = [[jaccard_similarity(main_article, article) for article in article_texts] for main_article in main_article_texts]\n",
    "print(\"JACCARD\\n\",jaccard_results)\n",
    "\n",
    "most_relevant_indices_jaccard = np.argmax(jaccard_results, axis=1)  # Get the index of the highest Jaccard score for each main article\n",
    "\n",
    "# Now, print the most relevant articles\n",
    "for i, index in enumerate(most_relevant_indices_jaccard):\n",
    "    print(f'Most relevant article to main article {i + 1}: Article ID {articles_df[\"article_id\"].iloc[index]}, Score: {jaccard_results[i][index]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-IDF\n",
      " [[0.4688665  0.24893798 0.25074102 0.24238164 0.18279797 0.00965629\n",
      "  0.00821295 0.02451711 0.0142275  0.         0.         0.\n",
      "  0.01308215 0.         0.00487419 0.10231106 0.0723798  0.03936148\n",
      "  0.00446883 0.         0.02044007 0.05737888 0.02312622 0.\n",
      "  0.0727262  0.16288353 0.02875954 0.00717856 0.         0.05234328]\n",
      " [0.01361154 0.0146547  0.00857552 0.00513095 0.         0.41302784\n",
      "  0.13265192 0.11786258 0.12705544 0.41613266 0.         0.\n",
      "  0.0162036  0.00640428 0.005092   0.01545356 0.0050568  0.01278371\n",
      "  0.02402345 0.02765484 0.07540858 0.02738852 0.09523189 0.05264441\n",
      "  0.02048835 0.005362   0.0415268  0.01660042 0.00518938 0.01222151]\n",
      " [0.         0.         0.         0.         0.         0.0280058\n",
      "  0.         0.         0.         0.01380417 0.58234194 0.12446177\n",
      "  0.1162371  0.16678141 0.2012572  0.01312809 0.         0.\n",
      "  0.         0.01130896 0.         0.00747022 0.         0.04411599\n",
      "  0.03419052 0.         0.         0.00781984 0.02259741 0.02344958]]\n",
      "Most relevant article to main article 1: Article ID 1, Score: 0.4688664951097442\n",
      "Most relevant article to main article 2: Article ID 10, Score: 0.4161326583152876\n",
      "Most relevant article to main article 3: Article ID 11, Score: 0.582341942319397\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Cosine Similarity\n",
    "tfidf_results = tfidf_cosine_similarity(article_texts, main_article_texts)\n",
    "print(\"tf-IDF\\n\", tfidf_results)\n",
    "\n",
    "most_relevant_indices_tfidf = np.argmax(tfidf_results, axis=1)  # Get the index of the highest TF-IDF score for each main article\n",
    "\n",
    "# Now, print the most relevant articles\n",
    "for i, index in enumerate(most_relevant_indices_tfidf):\n",
    "    print(f'Most relevant article to main article {i + 1}: Article ID {articles_df[\"article_id\"].iloc[index]}, Score: {tfidf_results[i][index]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOC2VEC\n",
      " [[0.9995862  0.99775237 0.9989814  0.9970363  0.998332   0.9879494\n",
      "  0.9884095  0.9915683  0.9869577  0.9872266  0.9751868  0.9781804\n",
      "  0.984184   0.97782177 0.9827848  0.9970736  0.99291795 0.99702764\n",
      "  0.9944109  0.98695755 0.9920814  0.99057436 0.9918618  0.9920186\n",
      "  0.993199   0.9963825  0.98819846 0.9895997  0.9892794  0.9941271 ]\n",
      " [0.990297   0.9830488  0.98728025 0.9807074  0.99015033 0.9989609\n",
      "  0.9954284  0.99689007 0.9969426  0.9991759  0.98153555 0.98524094\n",
      "  0.9884725  0.98435754 0.9868573  0.9910061  0.99344736 0.9925432\n",
      "  0.9960423  0.9932451  0.9966734  0.99286985 0.99690986 0.9957485\n",
      "  0.99477947 0.9854026  0.9937084  0.9919193  0.9920916  0.99210066]\n",
      " [0.98014253 0.96918756 0.97383493 0.9669619  0.98083764 0.9836617\n",
      "  0.9804289  0.9823265  0.9848832  0.98813766 0.9986988  0.99582475\n",
      "  0.9969885  0.998369   0.9955824  0.98202294 0.9802786  0.98088086\n",
      "  0.9859782  0.98045355 0.9823452  0.98111534 0.984378   0.98651236\n",
      "  0.98646516 0.97211176 0.9780463  0.9814614  0.9815984  0.9794254 ]]\n",
      "Most similar article to main article 1: Article ID 1, Score: 0.9995862245559692\n",
      "Most similar article to main article 2: Article ID 10, Score: 0.9991759061813354\n",
      "Most similar article to main article 3: Article ID 11, Score: 0.9986987709999084\n"
     ]
    }
   ],
   "source": [
    "# Doc2Vec Similarity\n",
    "doc2vec_model = train_doc2vec_model(article_texts)\n",
    "doc2vec_results = doc2vec_similarity(doc2vec_model, main_article_texts, article_texts)\n",
    "print(\"DOC2VEC\\n\",doc2vec_results)\n",
    "\n",
    "most_similar_indices_doc2vec = np.argmax(doc2vec_results, axis=1)  # Get the index of the highest similarity score for each main article\n",
    "\n",
    "# Now, print the most similar articles\n",
    "for i, index in enumerate(most_similar_indices_doc2vec):\n",
    "    print(f'Most similar article to main article {i + 1}: Article ID {articles_df[\"article_id\"].iloc[index]}, Score: {doc2vec_results[i][index]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE\n",
      " [[0.86069465 0.76420885 0.740463   0.76352227 0.7595467  0.19063026\n",
      "  0.21240547 0.1879184  0.2070472  0.2100079  0.14990816 0.16397636\n",
      "  0.17742212 0.14600855 0.17950413 0.6065383  0.36086747 0.48561922\n",
      "  0.05517561 0.21453068 0.22229032 0.32367644 0.22682057 0.19416277\n",
      "  0.19770594 0.65311444 0.26242983 0.26345915 0.24115716 0.2938951 ]\n",
      " [0.22951713 0.19375473 0.17456728 0.1695176  0.18252458 0.8090339\n",
      "  0.63131773 0.6268543  0.6458459  0.8375203  0.09526568 0.1528458\n",
      "  0.15937665 0.08314531 0.19652267 0.10925554 0.20254564 0.1765585\n",
      "  0.36833516 0.4398242  0.5382788  0.2800901  0.4004066  0.16178066\n",
      "  0.27254206 0.10683735 0.3967726  0.34346431 0.21737628 0.21110648]\n",
      " [0.13338564 0.15166816 0.10684954 0.09623087 0.15250912 0.17563823\n",
      "  0.11751913 0.09462082 0.12815009 0.09429544 0.91972935 0.8196043\n",
      "  0.7525368  0.84020984 0.7285485  0.14224401 0.09994093 0.1745681\n",
      "  0.1474084  0.10642788 0.11604126 0.26533926 0.08276558 0.28898537\n",
      "  0.24927774 0.13183558 0.08518702 0.16668099 0.14327794 0.15887241]]\n",
      "Most similar article to main article 1: Article ID 1, Score: 0.8606946468353271\n",
      "Most similar article to main article 2: Article ID 10, Score: 0.8375203013420105\n",
      "Most similar article to main article 3: Article ID 11, Score: 0.9197293519973755\n"
     ]
    }
   ],
   "source": [
    "# Universal Sentence Encoder (USE) Similarity\n",
    "use_results = use_similarity(use_model, main_article_texts, article_texts)\n",
    "print(\"USE\\n\",use_results)\n",
    "\n",
    "most_similar_indices = np.argmax(use_results, axis=1)  # Get the index of the highest similarity score for each main article\n",
    "\n",
    "# Now, you can print the most similar articles\n",
    "for i, index in enumerate(most_similar_indices):\n",
    "    print(f'Most similar article to main article {i + 1}: Article ID {articles_df[\"article_id\"].iloc[index]}, Score: {use_results[i][index]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT\n",
      " [[0.9734473  0.952684   0.9555909  0.93727356 0.9566747  0.7709882\n",
      "  0.7754092  0.7811339  0.76857364 0.77283347 0.70799327 0.7506634\n",
      "  0.74585706 0.74036765 0.74729294 0.9285645  0.82276434 0.881871\n",
      "  0.7599436  0.78573954 0.7939458  0.8056215  0.7691313  0.79088104\n",
      "  0.77249223 0.9178061  0.78859925 0.790879   0.7649845  0.7915998 ]\n",
      " [0.7437817  0.75796056 0.7642921  0.7440599  0.76163083 0.9705292\n",
      "  0.9090331  0.9000414  0.9213861  0.9762016  0.7652032  0.77109146\n",
      "  0.78228766 0.7657695  0.79718626 0.7354659  0.83528644 0.7883999\n",
      "  0.83507013 0.8684075  0.86954373 0.8121923  0.8663658  0.8247652\n",
      "  0.80878794 0.7150284  0.80993736 0.8401869  0.81717443 0.7820946 ]\n",
      " [0.6844796  0.6867629  0.6980556  0.6723521  0.70423406 0.75020003\n",
      "  0.70057404 0.7042655  0.7130294  0.74019855 0.9811785  0.9295438\n",
      "  0.9017242  0.95280284 0.8681864  0.687739   0.71629876 0.69612914\n",
      "  0.6955874  0.6975173  0.70476335 0.6943746  0.71442294 0.7603599\n",
      "  0.7312821  0.6888039  0.6989481  0.70684314 0.70105267 0.6939591 ]]\n",
      "Most similar article to main article 1: Article ID 1, Score: 0.973447322845459\n",
      "Most similar article to main article 2: Article ID 10, Score: 0.976201593875885\n",
      "Most similar article to main article 3: Article ID 11, Score: 0.9811785221099854\n"
     ]
    }
   ],
   "source": [
    "# BERT Cosine Similarity\n",
    "bert_results = bert_cosine_similarity(main_article_texts, article_texts)\n",
    "print(\"BERT\\n\", bert_results)\n",
    "\n",
    "# Assuming `bert_results` is your 2D array of BERT similarity scores\n",
    "most_similar_indices = np.argmax(bert_results, axis=1)  # Get the index of the highest similarity score for each main article\n",
    "\n",
    "# Now, you can print the most similar articles\n",
    "for i, index in enumerate(most_similar_indices):\n",
    "    print(f'Most similar article to main article {i + 1}: Article ID {articles_df[\"article_id\"].iloc[index]}, Score: {bert_results[i][index]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Most Similar (Jaccard)  Most Similar (TF-IDF)  Most Similar (Doc2Vec)  \\\n",
      "0                       1                      1                       1   \n",
      "1                       6                     10                      10   \n",
      "2                      11                     11                      11   \n",
      "\n",
      "   Most Similar (USE)  Most Similar (BERT)  \n",
      "0                   1                    1  \n",
      "1                  10                   10  \n",
      "2                  11                   11  \n"
     ]
    }
   ],
   "source": [
    "article_ids = articles_df['article_id'].tolist() \n",
    "\n",
    "comparison_data = {\n",
    "    'Most Similar (Jaccard)': [],\n",
    "    'Most Similar (TF-IDF)': [],\n",
    "    'Most Similar (Doc2Vec)': [],\n",
    "    'Most Similar (USE)': [],\n",
    "    'Most Similar (BERT)': []\n",
    "}\n",
    "\n",
    "# Example for processing the results (ensure that the matrices and lengths match your data)\n",
    "for i in range(3):  # Assuming you have three main articles\n",
    "    most_relevant_indices_jaccard = np.argmax(jaccard_results[i])  # Get the index of the highest score\n",
    "    most_relevant_indices_tfidf = np.argmax(tfidf_results[i])\n",
    "    most_relevant_indices_doc2vec = np.argmax(doc2vec_results[i])\n",
    "    most_relevant_indices_use = np.argmax(use_results[i])\n",
    "    most_relevant_indices_bert = np.argmax(bert_results[i])\n",
    "\n",
    "    comparison_data['Most Similar (Jaccard)'].append(article_ids[most_relevant_indices_jaccard])\n",
    "    comparison_data['Most Similar (TF-IDF)'].append(article_ids[most_relevant_indices_tfidf])\n",
    "    comparison_data['Most Similar (Doc2Vec)'].append(article_ids[most_relevant_indices_doc2vec])\n",
    "    comparison_data['Most Similar (USE)'].append(article_ids[most_relevant_indices_use])\n",
    "    comparison_data['Most Similar (BERT)'].append(article_ids[most_relevant_indices_bert])\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.int64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m overlaps \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m algo1, algo2 \u001b[38;5;129;01min\u001b[39;00m [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJaccard\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTF-IDF\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJaccard\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDoc2Vec\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJaccard\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSE\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJaccard\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBERT\u001b[39m\u001b[38;5;124m'\u001b[39m), \n\u001b[0;32m      7\u001b[0m                      (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTF-IDF\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDoc2Vec\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTF-IDF\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSE\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTF-IDF\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBERT\u001b[39m\u001b[38;5;124m'\u001b[39m), \n\u001b[0;32m      8\u001b[0m                      (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDoc2Vec\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSE\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDoc2Vec\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBERT\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBERT\u001b[39m\u001b[38;5;124m'\u001b[39m)]:\n\u001b[1;32m---> 10\u001b[0m     overlaps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjaccard-tfidf\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m calculate_overlap(\u001b[38;5;28mset\u001b[39m(most_relevant_indices_jaccard), \n\u001b[0;32m     11\u001b[0m                                                       \u001b[38;5;28mset\u001b[39m(most_relevant_indices_tfidf)) \u001b[38;5;66;03m# Adjust for each pair\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(overlaps)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.int64' object is not iterable"
     ]
    }
   ],
   "source": [
    "def calculate_overlap(set1, set2):\n",
    "    return len(set1.intersection(set2))\n",
    "\n",
    "# Assuming you have the results stored in sets\n",
    "overlaps = {}\n",
    "for algo1, algo2 in [('Jaccard', 'TF-IDF'), ('Jaccard', 'Doc2Vec'), ('Jaccard', 'USE'), ('Jaccard', 'BERT'), \n",
    "                     ('TF-IDF', 'Doc2Vec'), ('TF-IDF', 'USE'), ('TF-IDF', 'BERT'), \n",
    "                     ('Doc2Vec', 'USE'), ('Doc2Vec', 'BERT'), ('USE', 'BERT')]:\n",
    "    \n",
    "    overlaps['jaccard-tfidf'] = calculate_overlap(set(most_relevant_indices_jaccard), \n",
    "                                                      set(most_relevant_indices_tfidf)) # Adjust for each pair\n",
    "\n",
    "print(overlaps)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
