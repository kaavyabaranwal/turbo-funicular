{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SR8iwlt6sarH",
        "outputId": "ce265e55-4784-4f8a-d20a-3c1e39551754"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import tensorflow_hub as hub\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Download stopwords and tokenizer models if not available\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XIbLeAZtGX2",
        "outputId": "e23aba36-9a65-4784-9491-19fa6b75b4fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.17.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Ql8b2IuotfSX"
      },
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "articles_df = pd.read_csv('articles_dataset.csv')\n",
        "main_articles_df = pd.read_csv('main_articles.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bh2YM1gctvpF"
      },
      "outputs": [],
      "source": [
        "def jaccard_similarity(doc1, doc2):\n",
        "    tokens1 = set(word_tokenize(doc1.lower()))\n",
        "    tokens2 = set(word_tokenize(doc2.lower()))\n",
        "    intersection = tokens1.intersection(tokens2)\n",
        "    union = tokens1.union(tokens2)\n",
        "    return len(intersection) / len(union) if union else 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vuUI3Iq4t2Gv"
      },
      "outputs": [],
      "source": [
        "# Calculate TF-IDF cosine similarity\n",
        "def tfidf_cosine_similarity(articles, main_articles):\n",
        "    documents = articles + main_articles\n",
        "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "    similarity_matrix = cosine_similarity(tfidf_matrix[len(articles):], tfidf_matrix[:len(articles)])\n",
        "    return similarity_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "sKLyhRHa5CJ3"
      },
      "outputs": [],
      "source": [
        "def train_doc2vec_model(documents):\n",
        "    tagged_data = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]) for i, doc in enumerate(documents)]\n",
        "    model = Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
        "    model.build_vocab(tagged_data)\n",
        "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "    return model\n",
        "# Calculate Doc2Vec similarity\n",
        "def doc2vec_similarity(doc2vec_model, main_articles, articles):\n",
        "    similarities = []\n",
        "\n",
        "    # Get the vectors for the articles in the dataset\n",
        "    article_vectors = [doc2vec_model.infer_vector(word_tokenize(article.lower())) for article in articles]\n",
        "\n",
        "    for main_article in main_articles:\n",
        "        # Infer vector for the main article\n",
        "        main_vector = doc2vec_model.infer_vector(word_tokenize(main_article.lower()))\n",
        "\n",
        "        # Calculate cosine similarity between the main article vector and each article vector\n",
        "        sims = cosine_similarity([main_vector], article_vectors)[0]\n",
        "        similarities.append(sims)\n",
        "\n",
        "    return np.array(similarities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Rrj7OFLL5KuT"
      },
      "outputs": [],
      "source": [
        "# Load USE model\n",
        "use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "\n",
        "# Calculate USE similarity\n",
        "def use_similarity(use_model, main_articles, articles):\n",
        "    embeddings = use_model(articles + main_articles)\n",
        "    article_embeddings = embeddings[:len(articles)]\n",
        "    main_article_embeddings = embeddings[len(articles):]\n",
        "    similarity_matrix = cosine_similarity(main_article_embeddings, article_embeddings)\n",
        "    return similarity_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "FShneNfZ5W8r"
      },
      "outputs": [],
      "source": [
        "# Load BERT model and tokenizer\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Function to get BERT embeddings\n",
        "def bert_embedding(text):\n",
        "    inputs = bert_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    outputs = bert_model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
        "    return embeddings\n",
        "\n",
        "# Calculate BERT similarity\n",
        "def bert_cosine_similarity(main_articles, articles):\n",
        "    article_embeddings = torch.stack([bert_embedding(doc) for doc in articles])\n",
        "    main_article_embeddings = torch.stack([bert_embedding(doc) for doc in main_articles])\n",
        "    similarity_matrix = cosine_similarity(main_article_embeddings.detach().numpy(), article_embeddings.detach().numpy())\n",
        "    return similarity_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQAxAe035a-Z",
        "outputId": "084df9be-80e3-4b34-c3a6-a7ec68d7287b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JACCARD\n",
            " [[0.09774436090225563, 0.07633587786259542, 0.10655737704918032, 0.10077519379844961, 0.06299212598425197, 0.11023622047244094, 0.09836065573770492, 0.08527131782945736, 0.078125, 0.047244094488188976, 0.109375, 0.06153846153846154, 0.11764705882352941, 0.07936507936507936, 0.109375, 0.056, 0.06870229007633588, 0.104, 0.07936507936507936, 0.08333333333333333, 0.09322033898305085, 0.06349206349206349, 0.19827586206896552, 0.06611570247933884, 0.08, 0.07142857142857142, 0.06451612903225806, 0.088, 0.08064516129032258, 0.056], [0.08396946564885496, 0.11382113821138211, 0.09166666666666666, 0.078125, 0.11016949152542373, 0.10483870967741936, 0.08333333333333333, 0.14285714285714285, 0.06349206349206349, 0.075, 0.08661417322834646, 0.08064516129032258, 0.10256410256410256, 0.07317073170731707, 0.08661417322834646, 0.0847457627118644, 0.0625, 0.08943089430894309, 0.07317073170731707, 0.0859375, 0.06837606837606838, 0.0743801652892562, 0.08, 0.10619469026548672, 0.09166666666666666, 0.10084033613445378, 0.07563025210084033, 0.1092436974789916, 0.08333333333333333, 0.07563025210084033], [0.07142857142857142, 0.0743801652892562, 0.06896551724137931, 0.06504065040650407, 0.0782608695652174, 0.0743801652892562, 0.08849557522123894, 0.09322033898305085, 0.07627118644067797, 0.08928571428571429, 0.07377049180327869, 0.07627118644067797, 0.07964601769911504, 0.07758620689655173, 0.06504065040650407, 0.07079646017699115, 0.075, 0.08547008547008547, 0.08695652173913043, 0.09090909090909091, 0.08256880733944955, 0.08849557522123894, 0.07563025210084033, 0.102803738317757, 0.0782608695652174, 0.0782608695652174, 0.07079646017699115, 0.09649122807017543, 0.08849557522123894, 0.07079646017699115]]\n",
            "Most relevant article to main article 1: Article ID 23, Score: 0.19827586206896552\n",
            "Most relevant article to main article 2: Article ID 8, Score: 0.14285714285714285\n",
            "Most relevant article to main article 3: Article ID 24, Score: 0.102803738317757\n"
          ]
        }
      ],
      "source": [
        "# Prepare documents\n",
        "article_texts = articles_df['content'].tolist()\n",
        "main_article_texts = main_articles_df['content'].tolist()\n",
        "\n",
        "# Calculate similarities\n",
        "# Jaccard Similarity\n",
        "jaccard_results = [[jaccard_similarity(main_article, article) for article in article_texts] for main_article in main_article_texts]\n",
        "print(\"JACCARD\\n\",jaccard_results)\n",
        "\n",
        "most_relevant_indices_jaccard = np.argmax(jaccard_results, axis=1)  # Get the index of the highest Jaccard score for each main article\n",
        "\n",
        "# Now, print the most relevant articles\n",
        "for i, index in enumerate(most_relevant_indices_jaccard):\n",
        "    print(f'Most relevant article to main article {i + 1}: Article ID {articles_df[\"article_id\"].iloc[index]}, Score: {jaccard_results[i][index]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOMEcyOF5eJO",
        "outputId": "fbc9eacc-0dbf-4a43-d227-6ebce64f65c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf-IDF\n",
            " [[0.0217804  0.00481581 0.06499652 0.         0.0242082  0.01801648\n",
            "  0.02362924 0.03452992 0.01910044 0.         0.03238434 0.01004606\n",
            "  0.05569075 0.02081733 0.02705174 0.0043732  0.00449768 0.04023955\n",
            "  0.00869304 0.         0.03135368 0.00987148 0.20071333 0.00640803\n",
            "  0.00413691 0.01719533 0.01040261 0.00525467 0.00465664 0.        ]\n",
            " [0.02248223 0.08758718 0.01864782 0.         0.06888625 0.05153479\n",
            "  0.01015005 0.0958795  0.         0.00915999 0.02504953 0.02021558\n",
            "  0.         0.         0.         0.0201397  0.01926872 0.01605995\n",
            "  0.         0.         0.         0.01350968 0.01456487 0.03241623\n",
            "  0.01217191 0.02140081 0.02093306 0.01666734 0.0096415  0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.01272873 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.012517\n",
            "  0.         0.         0.         0.03351879 0.01596522 0.        ]]\n",
            "Most relevant article to main article 1: Article ID 23, Score: 0.20071333159037932\n",
            "Most relevant article to main article 2: Article ID 8, Score: 0.09587949563216153\n",
            "Most relevant article to main article 3: Article ID 28, Score: 0.033518794734484256\n"
          ]
        }
      ],
      "source": [
        "# TF-IDF Cosine Similarity\n",
        "tfidf_results = tfidf_cosine_similarity(article_texts, main_article_texts)\n",
        "print(\"tf-IDF\\n\", tfidf_results)\n",
        "\n",
        "most_relevant_indices_tfidf = np.argmax(tfidf_results, axis=1)  # Get the index of the highest TF-IDF score for each main article\n",
        "\n",
        "# Now, print the most relevant articles\n",
        "for i, index in enumerate(most_relevant_indices_tfidf):\n",
        "    print(f'Most relevant article to main article {i + 1}: Article ID {articles_df[\"article_id\"].iloc[index]}, Score: {tfidf_results[i][index]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3uKSewO5jO2",
        "outputId": "692bd93b-7bfe-40f7-bf1f-25863537c29c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DOC2VEC\n",
            " [[ 0.9973186   0.99713916  0.99738204  0.9976624   0.9984712   0.9975536\n",
            "   0.99732286  0.99533945  0.99650514  0.9958253   0.99805015  0.99614\n",
            "   0.99621695  0.9968534   0.9978405   0.9965933   0.9969848   0.9983051\n",
            "   0.99689424  0.99693084  0.99789953  0.99680096  0.9993549   0.99704283\n",
            "   0.9951497   0.9960851   0.99633783  0.99747366  0.99664384  0.9958559 ]\n",
            " [ 0.9949321   0.99701655  0.9963679   0.9963525   0.9981228   0.99760276\n",
            "   0.9971326   0.99611473  0.9960313   0.99670374  0.9976909   0.9962214\n",
            "   0.99619275  0.9951264   0.9972073   0.99591124  0.9959329   0.9965089\n",
            "   0.9969932   0.99749166  0.9974705   0.9955769   0.9977754   0.99706423\n",
            "   0.9952171   0.99637115  0.9960503   0.9973905   0.9964137   0.9964267 ]\n",
            " [-0.95434403 -0.9573092  -0.95415914 -0.96013534 -0.9570882  -0.9562215\n",
            "  -0.95451266 -0.9569132  -0.9554475  -0.9587459  -0.95767987 -0.9566163\n",
            "  -0.9581738  -0.95659184 -0.9563042  -0.957017   -0.9646126  -0.9563819\n",
            "  -0.9579512  -0.95547533 -0.95586973 -0.9566197  -0.95381874 -0.95666325\n",
            "  -0.9578258  -0.95723355 -0.96296495 -0.9566046  -0.9641749  -0.95657265]]\n",
            "Most similar article to main article 1: Article ID 23, Score: 0.999354898929596\n",
            "Most similar article to main article 2: Article ID 5, Score: 0.9981228113174438\n",
            "Most similar article to main article 3: Article ID 23, Score: -0.9538187384605408\n"
          ]
        }
      ],
      "source": [
        "# Doc2Vec Similarity\n",
        "doc2vec_model = train_doc2vec_model(article_texts)\n",
        "doc2vec_results = doc2vec_similarity(doc2vec_model, main_article_texts, article_texts)\n",
        "print(\"DOC2VEC\\n\",doc2vec_results)\n",
        "\n",
        "most_similar_indices_doc2vec = np.argmax(doc2vec_results, axis=1)  # Get the index of the highest similarity score for each main article\n",
        "\n",
        "# Now, print the most similar articles\n",
        "for i, index in enumerate(most_similar_indices_doc2vec):\n",
        "    print(f'Most similar article to main article {i + 1}: Article ID {articles_df[\"article_id\"].iloc[index]}, Score: {doc2vec_results[i][index]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuUl2Zcx5n71",
        "outputId": "cd657a7b-204f-4a96-fe28-4e1981d0fe73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "USE\n",
            " [[ 4.02789652e-01  2.29685292e-01  5.89172244e-01  2.72433937e-01\n",
            "   2.76163518e-01  3.52577507e-01  2.27192163e-01  2.11197421e-01\n",
            "   2.77394205e-01  3.36850405e-01  2.66531169e-01  1.08863637e-01\n",
            "   1.53085053e-01  3.25942546e-01  4.04314041e-01  1.66461170e-01\n",
            "   2.58218706e-01  2.27151513e-01  2.55472720e-01  1.64927751e-01\n",
            "   1.80166736e-01  1.76287889e-01  6.96550012e-01  1.05479538e-01\n",
            "   1.53027639e-01  2.55720317e-01  2.33985215e-01  2.05823690e-01\n",
            "   2.01395139e-01  2.53372282e-01]\n",
            " [ 1.59137890e-01  4.35144216e-01  1.69350490e-01  2.55951464e-01\n",
            "   2.71451473e-01  2.48794556e-01  2.70397365e-01  4.83301818e-01\n",
            "   1.86897218e-01  1.83519766e-01  1.72435641e-01  3.59144390e-01\n",
            "   1.76433563e-01  2.73521990e-01  2.93322057e-01  2.79748857e-01\n",
            "   2.66757965e-01  3.36545527e-01  2.76899457e-01  2.93179393e-01\n",
            "   2.09583342e-01  1.42115027e-01  1.19277760e-01  3.46502364e-01\n",
            "   2.62362838e-01  2.12230384e-01  3.48483205e-01  1.97962731e-01\n",
            "   2.85226732e-01  2.54298061e-01]\n",
            " [ 1.26950189e-01  8.88826549e-02  1.14761502e-01  6.32251650e-02\n",
            "   9.86033380e-02  1.12491876e-01  8.92699286e-02  5.17943501e-02\n",
            "   7.70535916e-02  3.98640707e-02  5.69635443e-02 -1.10499430e-02\n",
            "   5.83534166e-02  9.30915400e-02 -1.86028816e-02  3.73194963e-02\n",
            "   9.46034193e-02  1.46557391e-01  7.41704702e-02  2.70213615e-02\n",
            "   5.70097789e-02 -2.79705971e-04  6.69365078e-02  7.65411928e-02\n",
            "   5.81908301e-02  5.75025305e-02  3.62163968e-02 -1.27474964e-02\n",
            "   2.22702324e-02  1.01867884e-01]]\n",
            "Most similar article to main article 1: Article ID 23, Score: 0.6965500116348267\n",
            "Most similar article to main article 2: Article ID 8, Score: 0.4833018183708191\n",
            "Most similar article to main article 3: Article ID 18, Score: 0.14655739068984985\n"
          ]
        }
      ],
      "source": [
        "# Universal Sentence Encoder (USE) Similarity\n",
        "use_results = use_similarity(use_model, main_article_texts, article_texts)\n",
        "print(\"USE\\n\",use_results)\n",
        "\n",
        "most_similar_indices = np.argmax(use_results, axis=1)  # Get the index of the highest similarity score for each main article\n",
        "\n",
        "# Now, you can print the most similar articles\n",
        "for i, index in enumerate(most_similar_indices):\n",
        "    print(f'Most similar article to main article {i + 1}: Article ID {articles_df[\"article_id\"].iloc[index]}, Score: {use_results[i][index]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VISyEyD5sdq",
        "outputId": "8172b6c9-4656-4983-d0d6-6451504df69c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT\n",
            " [[0.8154442  0.76445353 0.91049993 0.8025923  0.80479634 0.8290639\n",
            "  0.75234413 0.79188144 0.8034493  0.7703412  0.79176444 0.7600287\n",
            "  0.7378598  0.8104273  0.812924   0.7594214  0.80785906 0.78706604\n",
            "  0.77083665 0.79576826 0.7492622  0.78824604 0.9157997  0.7850456\n",
            "  0.7753484  0.78648454 0.7519563  0.75925255 0.7702719  0.77745   ]\n",
            " [0.79281884 0.84747493 0.73648024 0.811086   0.7961228  0.8182022\n",
            "  0.79741544 0.8626021  0.7959354  0.7617864  0.8071481  0.81285673\n",
            "  0.7570687  0.8353065  0.7441153  0.80144566 0.81114435 0.8200847\n",
            "  0.8118242  0.84352535 0.76831675 0.8049456  0.7493039  0.83349544\n",
            "  0.79247713 0.80129474 0.80984044 0.7874184  0.80164754 0.8080899 ]\n",
            " [0.6877612  0.6562478  0.684982   0.68645597 0.665676   0.7076949\n",
            "  0.68225586 0.703962   0.7003481  0.66074795 0.68599594 0.6727643\n",
            "  0.6554333  0.71447873 0.6726414  0.68971395 0.69052154 0.6948788\n",
            "  0.662515   0.6842898  0.66977304 0.6788932  0.69185096 0.7039484\n",
            "  0.67381227 0.6828054  0.6364924  0.6844574  0.6633567  0.66524357]]\n",
            "Most similar article to main article 1: Article ID 23, Score: 0.9157996773719788\n",
            "Most similar article to main article 2: Article ID 8, Score: 0.8626021146774292\n",
            "Most similar article to main article 3: Article ID 14, Score: 0.7144787311553955\n"
          ]
        }
      ],
      "source": [
        "# BERT Cosine Similarity\n",
        "bert_results = bert_cosine_similarity(main_article_texts, article_texts)\n",
        "print(\"BERT\\n\", bert_results)\n",
        "\n",
        "# Assuming `bert_results` is your 2D array of BERT similarity scores\n",
        "most_similar_indices = np.argmax(bert_results, axis=1)  # Get the index of the highest similarity score for each main article\n",
        "\n",
        "# Now, you can print the most similar articles\n",
        "for i, index in enumerate(most_similar_indices):\n",
        "    print(f'Most similar article to main article {i + 1}: Article ID {articles_df[\"article_id\"].iloc[index]}, Score: {bert_results[i][index]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "zNp8EpjC50Lw"
      },
      "outputs": [],
      "source": [
        "article_ids = articles_df['article_id'].tolist()\n",
        "\n",
        "comparison_data = {\n",
        "    'Most Similar (Jaccard)': [],\n",
        "    'Most Similar (TF-IDF)': [],\n",
        "    'Most Similar (Doc2Vec)': [],\n",
        "    'Most Similar (USE)': [],\n",
        "    'Most Similar (BERT)': []\n",
        "}\n",
        "\n",
        "# Example for processing the results (ensure that the matrices and lengths match your data)\n",
        "for i in range(3):  # Assuming you have three main articles\n",
        "    most_relevant_indices_jaccard = np.argmax(jaccard_results[i])  # Get the index of the highest score\n",
        "    most_relevant_indices_tfidf = np.argmax(tfidf_results[i])\n",
        "    most_relevant_indices_doc2vec = np.argmax(doc2vec_results[i])\n",
        "    most_relevant_indices_use = np.argmax(use_results[i])\n",
        "    most_relevant_indices_bert = np.argmax(bert_results[i])\n",
        "\n",
        "    comparison_data['Most Similar (Jaccard)'].append(article_ids[most_relevant_indices_jaccard])\n",
        "    comparison_data['Most Similar (TF-IDF)'].append(article_ids[most_relevant_indices_tfidf])\n",
        "    comparison_data['Most Similar (Doc2Vec)'].append(article_ids[most_relevant_indices_doc2vec])\n",
        "    comparison_data['Most Similar (USE)'].append(article_ids[most_relevant_indices_use])\n",
        "    comparison_data['Most Similar (BERT)'].append(article_ids[most_relevant_indices_bert])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCVLzgXJ6nD_",
        "outputId": "7ec84fc9-51a6-478a-e222-6a8de442e772"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Article 1:\n",
            "  Most Similar (Jaccard): 23\n",
            "  Most Similar (TF-IDF): 23\n",
            "  Most Similar (Doc2Vec): 23\n",
            "  Most Similar (USE): 23\n",
            "  Most Similar (BERT): 23\n",
            "--------------------------------------------------\n",
            "Article 2:\n",
            "  Most Similar (Jaccard): 8\n",
            "  Most Similar (TF-IDF): 8\n",
            "  Most Similar (Doc2Vec): 5\n",
            "  Most Similar (USE): 8\n",
            "  Most Similar (BERT): 8\n",
            "--------------------------------------------------\n",
            "Article 3:\n",
            "  Most Similar (Jaccard): 24\n",
            "  Most Similar (TF-IDF): 28\n",
            "  Most Similar (Doc2Vec): 23\n",
            "  Most Similar (USE): 18\n",
            "  Most Similar (BERT): 14\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for index, row in comparison_df.iterrows():\n",
        "    print(f\"Article {index + 1}:\")\n",
        "    print(f\"  Most Similar (Jaccard): {row['Most Similar (Jaccard)']}\")\n",
        "    print(f\"  Most Similar (TF-IDF): {row['Most Similar (TF-IDF)']}\")\n",
        "    print(f\"  Most Similar (Doc2Vec): {row['Most Similar (Doc2Vec)']}\")\n",
        "    print(f\"  Most Similar (USE): {row['Most Similar (USE)']}\")\n",
        "    print(f\"  Most Similar (BERT): {row['Most Similar (BERT)']}\")\n",
        "    print(\"-\" * 50)  # Add a separator for clarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGTtZCFZ57hg",
        "outputId": "777a18d6-624f-43dc-da6d-b0b95401123d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'jaccard-tf-idf': 2, 'jaccard-doc2vec': 1, 'jaccard-use': 2, 'jaccard-bert': 2, 'tf-idf-doc2vec': 1, 'tf-idf-use': 2, 'tf-idf-bert': 2, 'doc2vec-use': 1, 'doc2vec-bert': 1, 'use-bert': 2}\n"
          ]
        }
      ],
      "source": [
        "def calculate_overlap(list1, list2):\n",
        "    return len(set(list1).intersection(set(list2)))\n",
        "\n",
        "# Assuming you have the results stored in lists\n",
        "overlaps = {}\n",
        "# Get the indices for all main articles for each algorithm\n",
        "most_relevant_indices_jaccard = [np.argmax(jaccard_results[i]) for i in range(len(jaccard_results))]\n",
        "most_relevant_indices_tfidf = [np.argmax(tfidf_results[i]) for i in range(len(tfidf_results))]\n",
        "most_relevant_indices_doc2vec = [np.argmax(doc2vec_results[i]) for i in range(len(doc2vec_results))]\n",
        "most_relevant_indices_use = [np.argmax(use_results[i]) for i in range(len(use_results))]\n",
        "most_relevant_indices_bert = [np.argmax(bert_results[i]) for i in range(len(bert_results))]\n",
        "\n",
        "for algo1, algo2 in [('Jaccard', 'TF-IDF'), ('Jaccard', 'Doc2Vec'), ('Jaccard', 'USE'), ('Jaccard', 'BERT'),\n",
        "                     ('TF-IDF', 'Doc2Vec'), ('TF-IDF', 'USE'), ('TF-IDF', 'BERT'),\n",
        "                     ('Doc2Vec', 'USE'), ('Doc2Vec', 'BERT'), ('USE', 'BERT')]:\n",
        "\n",
        "    # Fix: Use .replace to handle both 'tf-idf' and 'tfidf'\n",
        "    list1 = locals()[f\"most_relevant_indices_{algo1.lower().replace('-', '')}\"]\n",
        "    list2 = locals()[f\"most_relevant_indices_{algo2.lower().replace('-', '')}\"]\n",
        "\n",
        "    overlaps[f\"{algo1.lower()}-{algo2.lower()}\"] = calculate_overlap(list1, list2)\n",
        "\n",
        "print(overlaps)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
